---
title: "Models"
author: "Noelí D'Alessio"
date: "10/10/2020"
output: html_document
---

# A Simple Model

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(modelr)
options(na.action = na.warn)
```

sim1 is part of modelr, has two variables x and y

```{r}
ggplot(sim1, aes(x,y)) +
  geom_point()
```

```{r}
models <- tibble(
  a1 = runif(250, -20, 40),
  a2 = runif(250, -5, 5)
)

ggplot(sim1, aes(x, y)) +
  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha =1/4) +
  geom_point()
```

There are 250 models (predictions) here. We have to choose the one that fits the better. 
This distance is just the difference between the y value given by the model (the prediction), and the actual y value in the data (the response).

To compute this distance, we first turn our model family into an R function. This takes the model parameters and the data as inputs, and gives values predicted by the model as output:

```{r}
# y = b + m * x

# y = intercept + data$x * slope

model1 <- function(a, data){
  a[1] + data$x * a[2]      
}

model1(c(7, 1.5), sim1) 
# 7 + data$x * 1.5
# 7 + data$x * 1.5 etc.
```

Next, we need some way to compute an overall distance between the predicted and actual values.
One common way to do this in statistics to use the “root-mean-squared deviation”. We compute the difference between actual and predicted, square them, average them, and the take the square root. 

```{r}
measure_distance <- function(mod, data){
  diff <- data$y - model1(mod, data) #data$y - model
  sqrt(mean(diff ^ 2))
}

measure_distance(c(7, 1.5), sim1)
```

Now we can use purrr to compute the distance for all the models defined above. We need a helper function because our distance function expects the model as a numeric vector of length 2.

```{r}
sim1_dist <- function(a1, a2){
  #measure distance first estimates y with intercept and slop
  # and then measure the distance between the model and the y
  measure_distance(c(a1, a2), sim1) 
}

models <- models %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) 

models
```

Next, let’s overlay the 10 best models on to the data. I’ve coloured the models by -dist: this is an easy way to make sure that the best models (i.e. the ones with the smallest distance) get the brighest colours.

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") +
  geom_abline(
    aes(intercept = a1, slope = a2, color = -dist),
    data = filter(models, rank(dist) <= 10)
  )
```

```{r}
ggplot(models, aes(a1, a2)) +
  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```


Instead,  we could be more systematic and generate an evenly spaced grid of points (this is called a grid search)

```{r}
grid <- expand.grid(
  a1 = seq(-5, 20, length = 25),
  a2 = seq(1, 3, length = 25)
) %>% 
  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
  ggplot(aes(a1, a2)) +
  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = "red") +
  geom_point(aes(colour = -dist))
```

```{r}
ggplot(sim1, aes(x, y)) + 
  geom_point(size = 2, colour = "grey30") + 
  geom_abline(
    aes(intercept = a1, slope = a2, colour = -dist), 
    data = filter(grid, rank(dist) <= 10)
  )
```

You could imagine iteratively making the grid finer and finer until you narrowed in on the best model. But there’s a better way to tackle that problem: a numerical minimisation tool called Newton-Raphson search. The intuition of Newton-Raphson is pretty simple: you pick a starting point and look around for the steepest slope. You then ski down that slope a little way, and then repeat again and again, until you can’t go any lower. In R, we can do that with optim():

```{r}
best <- optim(c(0, 0), measure_distance, data = sim1)
best$par
```

```{r}
ggplot(sim1, aes(x, y)) +
  geom_point(size = 2, colour = "grey30") +
  geom_abline(intercept = best$par[1], slope = best$par[2])
```

R has a tool specifically designed for fitting linear models called lm():

```{r}
sim1_mod <- lm(y ~ x, data = sim1)
coef(sim1_mod)
```

# 23.2.1 Exercises

1. One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?

```{r}
sim1a <- tibble(
  x = rep(1:10, each = 3), 
  y = x * 1.5 + 6 + rt(length(x), df = 2) #rt pseudo number for t distributions
)
sim1a
```

```{r}
ggplot(sim1a, aes(x, y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

If we want to run many simulations, we can build a function, and add and id:

```{r}
simt <- function(i){
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rt(length(x), df = 2), #df is degree of freedom
    .id = i
  )
}

sims <- map_df(1:12, simt)

ggplot(sims, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "red") +
  facet_wrap(~.id, ncol = 4)
```

Same thing but with normal distributions:

```{r}
sim_norm <- function(i) {
  tibble(
    x = rep(1:10, each = 3),
    y = x * 1.5 + 6 + rnorm(length(x)),
    .id = i
  )
}

simdf_norm <- map_df(1:12, sim_norm)

ggplot(simdf_norm, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "red") +
  facet_wrap(~.id, ncol = 4)
```

In the Student's distribution there are more outliers thanin the normal distribution.

The Student’s t-distribution assigns a larger probability to values further from the center of the distribution than normal distribution.

```{r}
tibble(
  x = seq(-5, 5, length.out = 100),
  normal = dnorm(x),
  student_t = dt(x, df = 2)
) %>%
  pivot_longer(-x, names_to="distribution", values_to="density") %>%
  ggplot(aes(x = x, y = density, colour = distribution)) +
  geom_line()
```

The probability of being bigger than 2 is:

```{r}
pnorm(2, lower.tail = FALSE)
```

```{r}
pt(2, df = 2, lower.tail = FALSE)
```

2. One way to make linear models more robust is to use a different distance measure. For example, instead of root-mean-squared distance, you could use mean-absolute distance:

```{r}
measure_distance <- function(mod, data) {
  diff <- data$y - make_prediction(mod, data)
  mean(abs(diff))
}

# we need to make a make_prediction function that takes a numeric vector of length 2 (the intercept and the slope) and returns the prediction

make_prediction <- function(mod, data) {
  mod[1] + mod[2] * data$x
}

# sim1 data

best <- optim(c(0, 0), measure_distance, data = sim1a)
best$par
```

```{r}
# Using the sim1a data, while the parameters the minimize the least squares objective function are:

measure_distance_ls <-  function(mod, data){
  diff <- data$y - (mod[1] + mod[2] * data$x)
  sqrt(mean(diff^2))
}

best <- optim(c(0, 0), measure_distance_ls, data = sim1a)
best$par
```

In practice, I suggest not using optim() to fit this model, and instead using an existing implementation. The rlm() and lqs() functions in the MASS fit robust and resistant linear models.

3. One challenge with performing numerical optimization is that it’s only guaranteed to find a local optimum. What’s the problem with optimizing a three parameter model like this?



# 23.3 Visualising Models

# Predictions

modelr::data_grid(df)

```{r}
grid <- sim1 %>% 
  data_grid(x)

grid
```

modelr::add_predictions(df, model)

```{r}
grid <- grid %>% 
  add_predictions(sim1_mod)

grid
```


```{r}
ggplot(sim1, aes(x)) +
  geom_point(aes(y = y)) +
  geom_line(aes(y = pred), data = grid, color = "red", size = 1)
```

# Residuals

```{r}
sim1 <- sim1 %>%  #we have to use the original dataset
  add_residuals(sim1_mod)

sim1
```


There are a few different ways to understand what the residuals tell us about the model. One way is to simply draw a frequency polygon to help us understand the spread of the residuals:

```{r}
ggplot(sim1, aes(resid)) +
  geom_freqpoly(binwidth = 0.5)
```

This helps you calibrate the quality of the model: how far away are the predictions from the observed values? Note that the average of the residual will always be 0.

You’ll often want to recreate plots using the residuals instead of the original predictor. 

```{r}
ggplot(sim1, aes(x, resid)) +
  geom_ref_line(h =0) +
  geom_point()
```

This looks like random noise, suggesting that our model has done a good job of capturing the patterns in the dataset.

### 23.3.3 Exercises

1. Instead of using lm() to fit a straight line, you can use loess() to fit a smooth curve. Repeat the process of model fitting, grid generation, predictions, and visualisation on sim1 using loess() instead of lm(). How does the result compare to geom_smooth()?

```{r}
sim1_loess <- loess(y ~ x, data = sim1)
sim1_lm <- lm(y ~ x, data = sim1)

grid_loess <- sim1 %>% 
  add_predictions(sim1_loess)

sim1 <- sim1 %>% 
  add_residuals(sim1_lm) %>% 
  add_predictions(sim1_lm) %>% 
  add_residuals(sim1_loess) %>% 
  add_predictions(sim1_loess)
```

This plots the loess predictions. The loess produces a nonlinear, smooth line through the data.

```{r}
plot_sim1_loess <-
  ggplot(sim1, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = pred), data = grid_loess, colour = "red")
plot_sim1_loess
```

```{r}
plot_sim1_loess +
  geom_smooth(method = "loess", colour = "blue", se = FALSE, alpha = 0.20)

#geom_smooth uses loess()
```

We can plot the residuals (red), and compare them to the residuals from lm() (black). In general, the loess model has smaller residuals within the sample (out of sample is a different issue, and we haven’t considered the uncertainty of these estimates).

```{r}
ggplot(sim1, aes(x = x)) +
  geom_ref_line(h = 0) +
  geom_point(aes(y = resid)) +
  geom_point(aes(y = resid_loess), colour = "red")
```


2. add_predictions() is paired with gather_predictions() and spread_predictions(). How do these three functions differ?

3. What does geom_ref_line() do? What package does it come from? Why is displaying a reference line in plots showing residuals useful and important?

4. Why might you want to look at a frequency polygon of absolute residuals? What are the pros and cons compared to looking at the raw residuals?

